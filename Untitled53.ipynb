{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuKArVZpDC1N",
        "outputId": "e1eaaad0-01b2-40ef-9a4e-5acd79168910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.15)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Requirement already satisfied: litellm in /usr/local/lib/python3.10/dist-packages (1.52.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm) (3.10.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (1.54.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.9.2)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (1.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.32.3)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (0.8.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm) (0.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.54.0->litellm) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2024.8.30)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.7.0->litellm) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (4.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm) (0.24.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.54.0->litellm) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.54.0->litellm) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.54.0->litellm) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->litellm) (0.2.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "/bin/bash: line 1: pip install torch: command not found\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community\n",
        "!pip install litellm\n",
        "!pip install PyPDF2\n",
        "! pip install torch\n",
        "! pip install pypdf\n",
        "!pip install faiss-cpu\n",
        "!apt-get install -q -y poppler-utils # Install poppler-utils which contains pdfinfo\n",
        "!pip install pdf2image pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y tesseract-ocr # Install Tesseract OCR\n",
        "!apt-get install -y libtesseract-dev # Install Tesseract development files\n",
        "\n",
        "import pytesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'  # Set the path to the Tesseract executable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRBlHD6GEkpt",
        "outputId": "d815df0a-c0fe-48f5-d74d-b9fc1e75ca8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from litellm import completion\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import os\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "import concurrent.futures\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "DI7gsIpWDW7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to extract text from a single image page\n",
        "def extract_text_from_image(image, page_num):\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return {'content': text, 'page_number': page_num + 1}"
      ],
      "metadata": {
        "id": "IKNZbImBD_iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle PDF with images and extract text\n",
        "def get_text_from_file_tesseract(file_path):\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    texts = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(extract_text_from_image, img, idx) for idx, img in enumerate(images)]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            texts.append(future.result())\n",
        "\n",
        "    # Sorting by page number\n",
        "    texts = sorted(texts, key=lambda x: x['page_number'])\n",
        "\n",
        "    return texts"
      ],
      "metadata": {
        "id": "hiVyywhTEA1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PDF file and extract text\n",
        "pdf_path = r\"sampledata.pdf\"\n",
        "documents = get_text_from_file_tesseract(pdf_path)\n",
        "pprint(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTl7fHeqEDOL",
        "outputId": "5158da02-71d3-419c-e51a-779f2c0f07b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': 'Verizon Interview Experience for Software Developer\\n'\n",
            "             'Last Updated : 28 Nov, 2023 an (2) a\\n'\n",
            "             '\\n'\n",
            "             'Round 1:Online Test\\n'\n",
            "             '\\n'\n",
            "             'It consists of 3 sections:\\n'\n",
            "             '\\n'\n",
            "             '¢ Section 1 is essay writing which will give a topic and 20 '\n",
            "             'minutes u should write min 200 words to 300 words\\n'\n",
            "             '\\n'\n",
            "             '¢ Section 2 is coding there will 2 coding questions to solve one '\n",
            "             'is easy and the other is medium-level. Questions on math,\\n'\n",
            "             'arrays, strings, and maps will be asked mostly. | have solved 2 '\n",
            "             'questions.\\n'\n",
            "             '\\n'\n",
            "             '¢ Section 3 is like a puzzle round questions are infinite u '\n",
            "             'should answer as many as questions possible in a given time.\\n'\n",
            "             'this is a very crucial round to get selected | have done a good '\n",
            "             'job in it.\\n'\n",
            "             '\\n'\n",
            "             'Verdict: Cleared\\n'\n",
            "             'Round 2:Technical Interview\\n'\n",
            "             '\\n'\n",
            "             '* Questions on Java were asked mostly. | asked basic questions '\n",
            "             'on oops, threads.\\n'\n",
            "             '* Questions on SQL were asked. basic questions on group by, and '\n",
            "             'order by functions.\\n'\n",
            "             '¢ Asked how u would sort an array of numbers efficiently. | told '\n",
            "             'the bubble sort approach after that he asked me about\\n'\n",
            "             '\\n'\n",
            "             'the time complexity and whether it is optimized. then | answered '\n",
            "             'the merge sort approach he asked me to explain it\\n'\n",
            "             'step-by-step clearly.\\n'\n",
            "             '\\n'\n",
            "             '* Questions on Projects. Why have done this? Where it is mostly '\n",
            "             'used? were also asked.\\n'\n",
            "             '¢ | have answered most of the questions expect 1-2.\\n'\n",
            "             '\\n'\n",
            "             'Verdict: IndiaCleared\\n'\n",
            "             'Pole BCH aM led\\n'\n",
            "             '\\n'\n",
            "             '¢ Introduce yourself.\\n'\n",
            "             '¢ Are u willing to relocate anywhere in India?\\n'\n",
            "             '¢ Where do u see yourself after 2-3 years?\\n'\n",
            "             '\\x0c',\n",
            "  'page_number': 1},\n",
            " {'content': 'Google Interview Experience For Software Developer\\n'\n",
            "             'Last Updated : 02 May, 2024\\n'\n",
            "             '\\n'\n",
            "             '% O ?\\n'\n",
            "             '\\n'\n",
            "             'Reflecting on my recent experience interviewing with Google for '\n",
            "             'a software role has been nothing short of insightful and\\n'\n",
            "             'enlightening. Here are a few takeaways that might help!\\n'\n",
            "             '\\n'\n",
            "             '¢ Problem-solving and Logical thinking\\n'\n",
            "             '\\n'\n",
            "             '¢ Time and Space complexity (v important)\\n'\n",
            "             '\\n'\n",
            "             '¢ DS and Algorithms concepts understanding\\n'\n",
            "             '\\n'\n",
            "             'e Ability to solve problems in the given time frame\\n'\n",
            "             '\\n'\n",
            "             '¢ Code Comprehension (clean, proper steps & function calls)\\n'\n",
            "             '\\n'\n",
            "             '45 minutes interviews (over Google Meet)\\n'\n",
            "             'e¢ Round 1: Array+DP — LC medium\\n'\n",
            "             '¢ Round 2: Directed graph — LC hard + googleyness\\n'\n",
            "             '\\n'\n",
            "             'Both rounds went well, but in Round 2 | couldn’t optimise the '\n",
            "             'solution, just after that was googleyness round.\\n'\n",
            "             '\\n'\n",
            "             'The interviewers were not only highly skilled professionals but '\n",
            "             'also genuinely interested in understanding my thought\\n'\n",
            "             'process and approach to complex problems.\\n'\n",
            "             '\\n'\n",
            "             'The next day, | got a call from my HR that | couldn’t make it '\n",
            "             'and should apply after 6 months again.\\n'\n",
            "             '\\n'\n",
            "             'However, | am so grateful for the opportunity and look forward '\n",
            "             'to applying the lessons learned in future interviews.\\n'\n",
            "             '\\x0c',\n",
            "  'page_number': 2},\n",
            " {'content': 'NAVIN RC\\n'\n",
            "             '\\n'\n",
            "             'ao 91-8667451231 @& navinrc2003@gmail.com NavinRC GQ navinii11\\n'\n",
            "             '\\n'\n",
            "             ' \\n'\n",
            "             '\\n'\n",
            "             'Education\\n'\n",
            "             'Shiv Nadar university chennai 2021 - 2025\\n'\n",
            "             'BTech Artificial Intelligence and Data Science (CGPA: 7.5 / 10) '\n",
            "             'Chennai, TN\\n'\n",
            "             'SKV higher secondary school 2021\\n'\n",
            "             'Class 12 (percentage: 94.50 ) Namakkal, TN\\n'\n",
            "             'R.N.OXFORD MATRICULATION SCHOOL 2019\\n'\n",
            "             'Class 10 (percentage: 95.60 ) Namakkal, TN\\n'\n",
            "             '\\n'\n",
            "             'Technical Skills\\n'\n",
            "             '\\n'\n",
            "             ' \\n'\n",
            "             '\\n'\n",
            "             'Languages: Python, C++,Java, HTML, CSS,SQL, Javascript\\n'\n",
            "             '\\n'\n",
            "             'Technologies: Next js,Node.js,React.js, AWS Cloud\\n'\n",
            "             '\\n'\n",
            "             'Developer Tools: Juptyter, Github, Tableau, Visual Studio Code\\n'\n",
            "             '\\n'\n",
            "             'Python Libraries: numPy, pandas, scikit-Learn, tensorflow, '\n",
            "             'pytorch, keras\\n'\n",
            "             'Areas Of Expertise: Machine Learning, NLP, Operating '\n",
            "             'Systems,System Design\\n'\n",
            "             '\\n'\n",
            "             'Experience\\n'\n",
            "             '\\n'\n",
            "             ' \\n'\n",
            "             '\\n'\n",
            "             'Internship at AskJunior.ai - May 2024 - July 2024 QO\\n'\n",
            "             '¢ Developed a chatbot with RAG (Retrieval-Augmented Generation) '\n",
            "             'for enhanced PDF document interaction\\n'\n",
            "             '¢ Designed an UI inspired by Claude.ai and full-stack '\n",
            "             'functionality using Next.js, Clerk, PDF-lib, and Axios\\n'\n",
            "             '\\n'\n",
            "             '« Link for the chatbot - chatbot-n4in.vercel.app/\\n'\n",
            "             '\\n'\n",
            "             'Projects\\n'\n",
            "             '\\n'\n",
            "             ' \\n'\n",
            "             '\\n'\n",
            "             'Bank Marketing Insights Predictive Analysis of Bank Marketing '\n",
            "             'Campaign Data oO\\n'\n",
            "             '\\n'\n",
            "             'e Analyzed Portuguese bank campaign data, performing customer '\n",
            "             'segmentation with KMeans and\\n'\n",
            "             'Agglomerative Clustering\\n'\n",
            "             'e Applied PCA for dimensionality reduction and derived insights '\n",
            "             'using advanced statistical techniques for\\n'\n",
            "             '\\n'\n",
            "             'campaign effectiveness\\n'\n",
            "             'SUM-IT Youtube Summarizer using NLP, Text-to-Speech and OpenAl '\n",
            "             'API QO\\n'\n",
            "             'e Key Skills: Artificial Intelligence — chatGPT — WhisperAPI — '\n",
            "             'gTTS — Transformers — Streamlit —\\n'\n",
            "             'Text-to-Speech — Summarizer — NLP\\n'\n",
            "             'e SUM-IT, merging Streamlit with OpenAl’s Whisper API for '\n",
            "             'summarizing lengthy YouTube videos.\\n'\n",
            "             'e Added vocal renditions using ChatGPT API for better '\n",
            "             'accessibility.\\n'\n",
            "             '\\n'\n",
            "             'Emotion detection using NLP NLPTF-IDF , Tokenization,BI-LSTM oO\\n'\n",
            "             'e Developed a Bi-LSTM model for emotion detection, utilizing '\n",
            "             'tokenization, TF-IDF, word embeddings, and\\n'\n",
            "             'sentiment analysis to extract insights from unstructured text '\n",
            "             'data.\\n'\n",
            "             '\\n'\n",
            "             'Parallel Matrix Multiplication OPENMP,HPC oO\\n'\n",
            "             'e Implemented and optimized parallel matrix multiplication with '\n",
            "             'OpenMP, achieving 2x to 10x speedup\\n'\n",
            "             'through multi-core CPU utilization and performance validation.\\n'\n",
            "             '\\n'\n",
            "             'Certifications\\n'\n",
            "             '\\n'\n",
            "             ' \\n'\n",
            "             '\\n'\n",
            "             'ChatGPT Prompt Engineering for Developers DeepLearning.ai\\n'\n",
            "             'Learned techniques for crafting effective prompts to optimize '\n",
            "             'ChatGPT performance.\\n'\n",
            "             '\\n'\n",
            "             'Programming in Java, Joy of Computing using Python, Social '\n",
            "             'Networks NPTEL\\n'\n",
            "             'Guvi_WebDevelopment GUVI\\n'\n",
            "             '\\n'\n",
            "             'Html,css\\n'\n",
            "             '\\x0c',\n",
            "  'page_number': 3}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterTextSplitterWithPageNumbers(CharacterTextSplitter):\n",
        "    def __init__(self, chunk_size=1000, chunk_overlap=200, **kwargs):\n",
        "        # Call the superclass's __init__ method to initialize inherited attributes\n",
        "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap, **kwargs)\n",
        "        # Store chunk_size and chunk_overlap as instance attributes\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def split_documents(self, documents):\n",
        "        chunks = []\n",
        "        for doc in documents:\n",
        "            page_num = doc['page_number']\n",
        "            content = doc['content']\n",
        "\n",
        "            # Break the content into smaller chunks\n",
        "            while len(content) > 0:\n",
        "                # Take the first 1000 characters from the content\n",
        "                chunk = content[:self.chunk_size]\n",
        "                chunks.append({'content': chunk, 'page_number': page_num})\n",
        "\n",
        "                # Remove the processed chunk from the content\n",
        "                content = content[self.chunk_size - self.chunk_overlap:]  # Account for overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "# Define your text splitter with the chunk size and overlap\n",
        "text_splitter = CharacterTextSplitterWithPageNumbers(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "-cMQxzK_EIXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model for embedding\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk27aq_1FC_w",
        "outputId": "cc4fbcbe-fe20-4cb9-cba0-8ae3020eddfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for each text chunk\n",
        "def get_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text['content'], return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append({'embedding': embedding, 'page_number': text['page_number']})\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "2aaq-k7vFNm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create FAISS index\n",
        "embeddings = get_embeddings(texts)\n",
        "embedding_vectors = np.array([emb['embedding'] for emb in embeddings], dtype=np.float32)\n",
        "dimension = embedding_vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embedding_vectors)"
      ],
      "metadata": {
        "id": "026JcsEMFRR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the index to a file\n",
        "faiss.write_index(index, 'faiss_index.index')"
      ],
      "metadata": {
        "id": "90xmBONOFTop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most similar document to a query\n",
        "def find_most_similar_document(query):\n",
        "    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    D, I = index.search(np.array([query_embedding], dtype=np.float32), k=1)\n",
        "    most_similar_idx = I[0][0]\n",
        "    most_similar_document_content = texts[most_similar_idx]['content']\n",
        "    page_number = texts[most_similar_idx]['page_number']\n",
        "\n",
        "    return most_similar_document_content, page_number"
      ],
      "metadata": {
        "id": "CKjUgnKPFWys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggingface_hub import notebook_login\n",
        "\n",
        "#notebook_login()"
      ],
      "metadata": {
        "id": "fCvButTbgBr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary library if you haven't already\n",
        "#!pip install transformers\n",
        "\n",
        "# Download and load the Mistral model for generating answers\n",
        "#from transformers import AutoTokenizer, AutoModelForCausalLM  # Import necessary classes\n",
        "\n",
        "#mistral_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(mistral_model_name)\n",
        "#mistral_model = AutoModelForCausalLM.from_pretrained(mistral_model_name) # Use a different variable name to avoid overwriting"
      ],
      "metadata": {
        "id": "tgphB1dBgUxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "import os\n",
        "\n",
        "# Set environment variable for Replicate API key\n",
        "os.environ[\"REPLICATE_API_KEY\"] = \"r8_6clShA5Hu1RWuZk354YyqsKp8kA76ZL1hY7AR\"\n",
        "\n",
        "# Generate a response for a query using the Replicate LLaMA-3 model\n",
        "def get_response(query):\n",
        "    # Format the message as per the API requirements\n",
        "    response = completion(\n",
        "        model=\"replicate/meta/meta-llama-3-8b-instruct\",\n",
        "        messages=[{\"content\": query, \"role\": \"user\"}]\n",
        "    )\n",
        "\n",
        "    # Extract response text from the completion result\n",
        "    response_text = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "    return response_text\n"
      ],
      "metadata": {
        "id": "bQms8sueFZue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the answer from the extracted text\n",
        "def get_answer_from_pdf(query):\n",
        "    most_similar_document_content, page_number = find_most_similar_document(query)\n",
        "    prompt = f\"Based on the following content:\\n\\n{most_similar_document_content}\\n\\nAnswer the following question: {query}\"\n",
        "    answer = get_response(prompt)\n",
        "    return answer, page_number"
      ],
      "metadata": {
        "id": "g2VaOH-4FiG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "#os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "rXH5_iZ1Fkw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Navin from this pdf\"\n",
        "answer, page_number = get_answer_from_pdf(query)\n",
        "pprint(f\"Answer: {answer}, Found on Page: {page_number}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGMWkaCDFnEJ",
        "outputId": "74067bae-7d79-49bd-f6bf-8783cfb127b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Answer: \\n'\n",
            " '\\n'\n",
            " 'Based on the provided content, Navin is a student pursuing his B.Tech in '\n",
            " 'Artificial Intelligence and Data Science at Shiv Nadar University, Chennai. '\n",
            " 'He has also completed his Class 12 and Class 10 from SKV Higher Secondary '\n",
            " 'School and R.N.Oxford Matriculation School, respectively., Found on Page: 3')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AI_9mFFFt1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}